{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9a8f58-c9b4-4631-a6eb-958708ee2005",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <b>TUTORIAL: Speech to text - Speaker diarization and timestamped transcript</b>\n",
    "\n",
    "![title](images/main_sketch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23841af-e072-43a4-9ed9-28dfbf38f6fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is the second of a series of tutorials. We strongly recommend you to **read the first episode** __[Speech to text - Basics]__ before doing this one, since we will use functions defined and explained in the previous episode. \n",
    "\n",
    "## Objective\n",
    "\n",
    "The different steps of this second tutorial are as follow:\n",
    "<ul>\n",
    "    <li>Requirements</li>\n",
    "    <li>Speaker diarization (Differentiate speakers)</li>\n",
    "    <li>Create subtitles for videos, movies, ... (.srt file) </li>\n",
    "</ul>\n",
    "\n",
    "## Going further\n",
    "**Next tutorial** will show you how to:\n",
    "\n",
    "<ul>\n",
    "    <li>Compare models and choose the best one</li>\n",
    "    <li>Build a complete Streamlit application to make your code interactive.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735f36b-49eb-49e5-8859-9ebdeffb05ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Requirements\n",
    "\n",
    "*This notebook has been created on **AI Notebooks** with the \"Miniconda- conda-py39-cuda11.2-v22-4\" image.*\n",
    "### A - Install and import dependencies\n",
    "As the first step, let's set up the structure of our project. Several librairies need to be installed and imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1034a6-e37d-49d7-9f00-f9589ad8665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace8bfb-1a26-4188-879c-54e01bd76fe5",
   "metadata": {},
   "source": [
    "<br>⚠️ After installing these librairies, you may get <b>errors</b> if you try to import them directly. \n",
    "<br>To avoid that, just <b>restart the kernel</b> before the imports to make sure everything works fine. (Can be done by clicking the arrow in a circle shape above, or clicking Menu -> Kernel -> Restart kernel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f293431a-4ee8-4155-8e31-0ead9d1a90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sound from YouTube URL\n",
    "import youtube_dl\n",
    "from youtube_dl import DownloadError\n",
    "\n",
    "# Manipulate sound\n",
    "import librosa\n",
    "from pydub import AudioSegment, silence\n",
    "from IPython.display import Audio\n",
    "import audioread\n",
    "\n",
    "# Load models & metrics to evaluate results\n",
    "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration, Wav2Vec2Tokenizer, Wav2Vec2ForCTC \n",
    "import torch\n",
    "from torchmetrics import WordErrorRate\n",
    "\n",
    "# Others (Text & time process, system navigation)\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509649d7-67ac-4d67-b9c4-c89707bb86ab",
   "metadata": {},
   "source": [
    "### B - Load Pretrained models\n",
    "We can now load the Wav2Vec2 model from Hugging Face (base 960h version). This model will perform the STT action.\n",
    "This model is only one of many. We will in a next tutorial compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4866323e-9c56-4893-bb51-21ec1a314234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:750: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "stt_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "stt_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b4c9e-4435-43cc-aa65-b231ba4eb364",
   "metadata": {},
   "source": [
    "⚠️ Messages in red as below may appear throughout the execution of the code.\n",
    "<br>Do not worry, these warnings are generated by the libraries and do not disturb the execution of the code.\n",
    "\n",
    "### C - Create a folder to store the sounds we transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f565c8-42f9-407a-88bf-ab336fe9f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/workspace/audio_chunks\" \n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab56c72-5088-499b-bcc9-3899e9fe01f2",
   "metadata": {},
   "source": [
    "### D - Import code from previous tutorial\n",
    "All this code has been explained in the first tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ba03db-375a-4dfc-9143-ef54541fd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_part(filename, stt_model, stt_tokenizer, myaudio, sub_start, sub_end, index):\n",
    "    \"\"\"\n",
    "    Transcribe an audio between a sub_start and a sub_end value (s)\n",
    "    :param filename: name of the audio file\n",
    "    :param stt_model: Wav2Vec2 Model (Speech to text model)\n",
    "    :param stt_tokenizer: Wav2Vec2’s Tokenizer (Speech to text model's tokenizer)\n",
    "    :param myaudio: AudioSegment file\n",
    "    :param sub_start: start value (s) of the considered audio part to transcribe\n",
    "    :param sub_end: end value (s) of the considered audio part to transcribe\n",
    "    :param index: audio file counter\n",
    "\n",
    "    :return: transcript of the considered audio (only in uppercases, so we add lower() to make the reading easier)\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            new_audio = myaudio[sub_start:sub_end]  # Works in milliseconds\n",
    "            path = \"/workspace/audio_chunks/\" + filename[:-3] + str(index) + \".mp3\"\n",
    "            new_audio.export(path)  # Exports to a mp3 file in the current path.\n",
    "            input_audio, _ = librosa.load(path, sr=16000)\n",
    "\n",
    "            input_values = stt_tokenizer(input_audio,\n",
    "                                         return_tensors=\"pt\").to(device).input_values  # Add padding to separate audio, padding=\"longest\", link https://huggingface.co/transformers/v3.0.2/preprocessing.html\n",
    "\n",
    "            logits = stt_model.to(device)(input_values).logits\n",
    "            prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Decode & add to our caption string\n",
    "            transcript = stt_tokenizer.batch_decode(prediction)[0]\n",
    "            return transcript.lower()\n",
    "\n",
    "    except audioread.NoBackendError:\n",
    "        # Means we have a chunk with a [value1 : value2] case with value1>value2\n",
    "        print(\"Sorry, seems we have a problem on our side. Please change start & end values.\")\n",
    "        \n",
    "        # Stop \n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b119ff3-9b7b-44d5-8a01-1491eb497446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_silences(audio):\n",
    "    # Get Decibels (dB) relative to Full Scale so silence detections depends on the audio instead of a fixed value\n",
    "    dbfs = audio.dBFS\n",
    "    \n",
    "    # Get silences timestamps > 750ms \n",
    "    silence_list = silence.detect_silence(audio, min_silence_len=750, silence_thresh=dbfs-14)\n",
    "    return silence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b958e-3682-4529-a7eb-8f649e5f8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_middle_values(silence_list):\n",
    "    # Replace timestamps by their middle value\n",
    "    new_silence_list=[]\n",
    "    for timestamp in silence_list:\n",
    "        new_silence_list.append(timestamp[0]+(timestamp[1]-timestamp[0])/2)\n",
    "    return new_silence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b12199a-9066-4d78-af34-5ba9b1cb3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silences_distribution(silence_list, min_space, max_space, start, end, srt_token = False):\n",
    "    \"\"\"\n",
    "    We keep each silence value if it is sufficiently distant from its neighboring values, without being too much\n",
    "    :param silence_list: List with silence intervals\n",
    "    :param min_space: Minimum temporal distance between two silences\n",
    "    :param max_space: Maximum temporal distance between two silences\n",
    "    :param start: int value (s) \n",
    "    :param end: int value (s) \n",
    "    :param srt_token: Enable/Disable generate srt file (choice fixed by user)\n",
    "    :return: list with equally distributed silences\n",
    "    \"\"\"\n",
    "    \n",
    "    # If starts != 0, we need to adjust end value since silence detection is performed on the cropped audio (and not on the original audio)\n",
    "    end -= start\n",
    "    start = 0\n",
    "    end *= 1000\n",
    "    \n",
    "    # Adding first element\n",
    "    newsilence = [silence_list[0]]\n",
    "    min_desired_value = silence_list[0]\n",
    "    max_desired_value = silence_list[0]\n",
    "    nb_values = len(silence_list)-1  # Minus one bc we have added the first element, so there is one less\n",
    "\n",
    "    # end - newsilence[-1] > min_space\n",
    "    while nb_values != 0:\n",
    "        max_desired_value += max_space\n",
    "        silence_window = list(filter(lambda x: min_desired_value < x <= max_desired_value, silence_list))\n",
    "        if silence_window != []:\n",
    "            if srt_token:\n",
    "                nearest_value = min(silence_window, key=lambda x: abs(x - min_desired_value))  # 10 = 20/2\n",
    "                nb_values -= silence_window.index(nearest_value) + 1  # (begins at 0, so we add 1)\n",
    "            else:\n",
    "                nearest_value = min(silence_window, key=lambda x: abs(x - max_desired_value))  # 10 = 20/2\n",
    "                # Max value index = len of the list\n",
    "                nb_values -= len(silence_window)\n",
    "\n",
    "            if nearest_value - newsilence[-1] <= min_space:\n",
    "                # If we have near values, we change the last value to the new one\n",
    "                # But we need to check with the two values before if gap is > max_space (mustn't)\n",
    "                if len(newsilence) >= 2:\n",
    "                    if nearest_value - newsilence[-2] > max_space:\n",
    "                        # Gap Too important\n",
    "                        newsilence.append(nearest_value)\n",
    "                    else:\n",
    "                        newsilence[-1] = nearest_value\n",
    "                else:\n",
    "                    # There is only one value for now, we can modify it\n",
    "                    newsilence.append(nearest_value)\n",
    "            else:\n",
    "                # If gap > min_space\n",
    "                newsilence.append(nearest_value)\n",
    "\n",
    "        else:\n",
    "            newsilence.append(newsilence[-1] + max_space)  # We add the max_space to avoid multiple audio cutting\n",
    "\n",
    "        min_desired_value = newsilence[-1]\n",
    "        max_desired_value = newsilence[-1]\n",
    "\n",
    "    final_value = end\n",
    "    if final_value - newsilence[-1] > min_space:\n",
    "        # Gap > Min Space\n",
    "        if final_value - newsilence[-1] < max_space:\n",
    "            newsilence.append(final_value)\n",
    "        else:\n",
    "            # Gap too important => Automatic max_space cut till the end\n",
    "            newsilence = generate_regular_split(newsilence, int(end), min_space, max_space)\n",
    "    else:\n",
    "        # Gap < Min Space <=> Final value and last value of new silence are too close\n",
    "        if len(newsilence) >= 2:\n",
    "            if final_value - newsilence[-2] <= max_space:\n",
    "                # Replace if gap is not too important\n",
    "                newsilence[-1] = final_value\n",
    "            else:\n",
    "                newsilence.append(final_value)\n",
    "\n",
    "        else:\n",
    "            if final_value - newsilence[-1] <= max_space:\n",
    "                # Replace if gap is not too important\n",
    "                newsilence[-1] = final_value\n",
    "            else:\n",
    "                newsilence.append(final_value)\n",
    "\n",
    "    if len(newsilence) >= 2:\n",
    "        # Replace or Adding start\n",
    "        if newsilence[0] - start < max_space:\n",
    "            # Replace to avoid a transcription from 0 to 0.9s (first silence)\n",
    "            newsilence[0] = start\n",
    "        else:\n",
    "            #Adding\n",
    "            newsilence.insert(0, start)\n",
    "    else:\n",
    "        newsilence.insert(0, start)\n",
    "\n",
    "    return newsilence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28ddefbe-4c04-4565-8818-32300321680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regular_split(time_list, end, min_space, max_space):\n",
    "    \"\"\"\n",
    "    Add automatic \"time cuts\" to mylist till end value depending on min_space and max_space value\n",
    "    :param time_list: silence time list\n",
    "    :param end: int value (s)\n",
    "    :param min_space: Minimum temporal distance between two silences\n",
    "    :param max_space: Maximum temporal distance between two silences\n",
    "    :return: list with automatic time cuts\n",
    "    \"\"\"\n",
    "    for i in range(int(time_list[-1]), end, max_space):\n",
    "        value = i + max_space\n",
    "        if value < end:\n",
    "            time_list.append(value)\n",
    "\n",
    "    # Fixing last automatic cut\n",
    "    # Small Gap (395 000, with end = 400 000)\n",
    "    if end - time_list[-1] < min_space:\n",
    "        time_list[-1] = end\n",
    "    else:\n",
    "        # Important gap ( 311 000 then 356 000, with end = 400 000, can't have 311k to 400k)\n",
    "        time_list.append(end)\n",
    "    return time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1542b52-aaf2-4c66-9390-bb269845e6a7",
   "metadata": {},
   "source": [
    "# Step 2 - Differentiate speakers\n",
    "\n",
    "Now that you know the basics of speech transcription, you might be interested in the diarization. It consists in partitioning the transcript among the several persons who speak in a conversation and then improves the readability of a speech transcription.\n",
    "<br><br>Indeed, you will get a transcript that looks like this: \n",
    "<br>\n",
    "<blockquote>\n",
    "    \n",
    "Client1: What is AI Notebooks?\n",
    "    \n",
    "Manager: This product is a convenient, easy to use and cost effective way to adress all your data science projects. \n",
    "\n",
    "Client2: Will our projects be secured if they are stored in your Cloud?\n",
    "\n",
    "Manager: It is one of our main goals in the company. As AI Notebooks is built on OVH Public Cloud, the product benefits from the A grade which ensures the security of your datas and a respect of your privacy.\n",
    "\n",
    "Client2: Perfect, thank you!\n",
    "</blockquote>\n",
    "\n",
    "### A - Load model\n",
    "This task is hard to perform, espacially when people are speaking at the same time. But it can be done thanks to pyannote library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7cf6e3-6651-419a-a7b0-ad14956163d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/.cache/torch/hub/pyannote_pyannote-audio_master\n",
      "Using cache found in /workspace/.cache/torch/hub/pyannote_pyannote-audio_master\n",
      "Using cache found in /workspace/.cache/torch/hub/pyannote_pyannote-audio_master\n",
      "Using cache found in /workspace/.cache/torch/hub/pyannote_pyannote-audio_master\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/pyannote/audio/embedding/approaches/arcface_loss.py:170: FutureWarning: The 's' parameter is deprecated in favor of 'scale', and will be removed in a future release\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/pyannote/audio/features/pretrained.py:156: UserWarning: Model was trained with 4s chunks and is applied on 2s chunks. This might lead to sub-optimal results.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /workspace/.cache/torch/hub/pyannote_pyannote-audio_master\n"
     ]
    }
   ],
   "source": [
    "# Load diarization model\n",
    "dia_pipeline = torch.hub.load('pyannote/pyannote-audio', 'dia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec8731-165e-4d62-bc45-8cfe53986d20",
   "metadata": {},
   "source": [
    "<br>⚠️ Here again, messages in red as above may appear throughout the execution of the code.\n",
    "<br>Do not worry, these warnings are generated by the libraries we use and do not disturb the execution of the code.\n",
    "<br><br>\n",
    "\n",
    "### C - Load an audio file\n",
    "Studying Obama's speech from first tutorial would not be interesting here as there is only one speaker in it. Try to select an audio file where several persons are speaking. This file can either be a local one, or extracted from a YouTube video like we have seen in the first tutorial.\n",
    "\n",
    "The longer your audio is, the longer the result will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2775365-e780-4280-83f1-c325ccdb13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an audio file where there are several speakers \n",
    "filename = \"Business English B1 - B2 Participating in meetings 1 (mp3cut.net).mp3\"\n",
    "\n",
    "# Create an audio segment from it\n",
    "conversation_audio = AudioSegment.from_file(\"/workspace/Speech_to_Text/sounds/\" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a58d9-4d8c-467b-97f6-0d64d25b0d6a",
   "metadata": {},
   "source": [
    "### D - Convert audio file format\n",
    "Once we have the diarization model loaded and our audio file, we need to **verify its format**. Indeed, pyannote's diarization <b>only accepts today </b>`.wav` format. If you send a `.mp3` or `.mp4` in the diarization algorithm, you will get errors.\n",
    "<br>But do not worry, this audio format convertion can be easily done with pydub as follow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1b83a5f-4379-4aee-9a64-79fe0acee6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_wav(aud_seg, filename):\n",
    "    \"\"\"\n",
    "    Convert a mp3/mp4 in a wav format\n",
    "    Needs to be modify if you want to convert a format which contains less or more than 3 letters\n",
    "\n",
    "    :param aud_seg: pydub.AudioSegment\n",
    "    :param filename: name of the file\n",
    "    :return: name of the converted file\n",
    "    \"\"\"\n",
    "    filename =  filename[:-3] + \"wav\" #folder\n",
    "    file_path = \"/workspace/Speech_to_Text/sounds/\" + filename\n",
    "    aud_seg.export(file_path, format=\"wav\")\n",
    "\n",
    "    newaudio = AudioSegment.from_file(file_path)\n",
    "\n",
    "    return newaudio, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e030ca7a-fb9c-4548-96c4-b8d03c09094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mp3/mp4 to wav (Differentiate speakers mode only accepts wav files)\n",
    "if filename.endswith((\".mp3\", \".mp4\")):\n",
    "    conversation_audio, filename = convert_file_to_wav(conversation_audio, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c4175-150b-41b1-b1b4-d5f1c93f8c8f",
   "metadata": {},
   "source": [
    "### E - Get diarization\n",
    "Once the audio is converted, we can send the `.wav` audio to pyannote's diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1477d6f1-104e-4a8a-84aa-3e79e3976d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarization(dia_pipeline, filename):\n",
    "    \"\"\"\n",
    "    Diarize an audio (find numbers of speakers, when they speak, ...)\n",
    "    :param dia_pipeline: Pyannote's library diarization model\n",
    "    :param filename: name of a wav audio file\n",
    "    :return: str list containing audio's diarization time intervals\n",
    "    \"\"\"\n",
    "    # Get diarization of the audio\n",
    "    diarization = dia_pipeline({'audio': \"/workspace/Speech_to_Text/sounds/\" + filename})\n",
    "    \n",
    "    # Rename default speakers' names (Default is A, B, ...), we want Speaker0, Speaker1, ...\n",
    "    listmapping = diarization.labels()\n",
    "    listnewmapping = []\n",
    "\n",
    "    number_of_speakers = len(listmapping)\n",
    "    for i in range(number_of_speakers):\n",
    "        listnewmapping.append(\"Speaker\" + str(i))\n",
    "\n",
    "    mapping_dict = dict(zip(listmapping, listnewmapping))\n",
    "    diarization.rename_labels(mapping_dict,\n",
    "                              copy=False)  # False so we don't create a new annotation, we replace the actuel one\n",
    "    return diarization, number_of_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dad8df4c-8aba-4732-9e6d-597979941f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.9/site-packages/pyannote/audio/features/utils.py:179: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  y = librosa.core.resample(y[:, 0], sample_rate, self.sample_rate)[:, None]\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/pyannote/audio/features/utils.py:179: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  y = librosa.core.resample(y[:, 0], sample_rate, self.sample_rate)[:, None]\n"
     ]
    }
   ],
   "source": [
    "# Differentiate speakers process\n",
    "diarization, number_of_speakers = get_diarization(dia_pipeline, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e9e0a-36ff-48d2-9986-e8572b2dfe15",
   "metadata": {},
   "source": [
    "Let's have a look to the results of diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf341088-46f9-473d-b051-1235128e4bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00:00:00.210 -->  00:00:01.493] L Speaker1\n",
      "[ 00:00:01.694 -->  00:00:02.440] A Speaker0\n",
      "[ 00:00:02.747 -->  00:00:03.133] B Speaker0\n",
      "[ 00:00:03.660 -->  00:00:05.464] C Speaker0\n",
      "[ 00:00:06.108 -->  00:00:07.836] D Speaker0\n",
      "[ 00:00:09.036 -->  00:00:10.337] E Speaker0\n",
      "[ 00:00:10.535 -->  00:00:12.612] F Speaker0\n",
      "[ 00:00:14.256 -->  00:00:14.753] G Speaker0\n",
      "[ 00:00:15.069 -->  00:00:20.923] H Speaker0\n",
      "[ 00:00:21.308 -->  00:00:22.356] I Speaker0\n",
      "[ 00:00:22.896 -->  00:00:23.798] J Speaker0\n",
      "[ 00:00:24.224 -->  00:00:25.391] K Speaker0\n",
      "[ 00:00:25.621 -->  00:00:28.895] M Speaker1\n",
      "[ 00:00:29.414 -->  00:00:31.279] N Speaker1\n",
      "[ 00:00:31.611 -->  00:00:35.061] O Speaker1\n",
      "[ 00:00:35.575 -->  00:00:40.211] P Speaker1\n",
      "[ 00:00:40.498 -->  00:00:42.015] Q Speaker1\n",
      "[ 00:00:42.781 -->  00:00:43.532] R Speaker1\n",
      "[ 00:00:43.740 -->  00:00:44.475] S Speaker1\n",
      "[ 00:00:44.764 -->  00:00:47.194] T Speaker1\n",
      "[ 00:00:47.530 -->  00:00:49.226] U Speaker1\n",
      "[ 00:00:49.673 -->  00:00:50.322] V Speaker1\n",
      "[ 00:00:50.625 -->  00:00:53.034] W Speaker1\n",
      "[ 00:00:53.638 -->  00:00:54.835] X Speaker1\n"
     ]
    }
   ],
   "source": [
    "print(diarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c6f91-65ca-4e36-84ac-f0af7ac4400a",
   "metadata": {},
   "source": [
    "The list is long, but you can visualize the result in a few seconds by typing the name of the variable as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac79e1cb-e316-4c6b-b22e-e1751e04f9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x7f218c3fa550>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f937219-dcbd-4b1c-b10d-ac99167476f1",
   "metadata": {},
   "source": [
    "Pyannote does the job! As you can see, two speakers have been detected in our audio. They are represented on the sketch above in red and blue and we get the timestamps of each speaking time. Then, we know who speaks when!\n",
    "As in the first episode, the idea would be to merge the timestamps of each speakers if they don't exceed a max_space value, to reduce the number of audio chunks and avoid memory problems. \n",
    "\n",
    "<br> To do that, we first need to convert the result of diarization into manipulable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7aa8270-cdb7-47e6-9016-07e07a13d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dia_results_to_timedelta(diarization_result, start):\n",
    "    \"\"\"\n",
    "    Extract from Diarization result the given speakers with their respective speaking times and transform them in pandas timedelta objects\n",
    "    :param diarization_result: result of diarization\n",
    "    :return: list with timedelta intervals and its respective speaker\n",
    "    \"\"\"\n",
    "    \n",
    "    segments = diarization_result.for_json()[\"content\"]\n",
    "    diarization_timestamps = []\n",
    "    for sample in segments:\n",
    "\n",
    "        new_seg = [pd.Timedelta(seconds = round(sample[\"segment\"][\"start\"] + start, 2)), pd.Timedelta(seconds = round(sample[\"segment\"][\"end\"] + start, 2)), sample[\"label\"]]\n",
    "        # Start and end = speaking duration\n",
    "        # label = who is speaking\n",
    "        diarization_timestamps.append(new_seg)\n",
    "\n",
    "    return diarization_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b7f023-509a-4366-877d-39d1f220d5f8",
   "metadata": {},
   "source": [
    "Then, we merge the segments that follow each other and that mention the same speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1174cdcf-4a72-4f3d-b7fb-fd37a989978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_speaker_times(diarization_timestamps, max_space, srt_token = False):\n",
    "    \"\"\"\n",
    "    Merge near times for each detected speaker. (Same speaker during 1-2s and 3-4s -> Same speaker during 1-4s)\n",
    "    :param diarization_timestamps: diarization list without overlaps\n",
    "    :param srt_token: Enable/Disable generate srt file (choice fixed by user)\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    if not srt_token:\n",
    "        threshold = pd.Timedelta(seconds=max_space/1000)\n",
    "\n",
    "        index = 0\n",
    "        length = len(diarization_timestamps) - 1\n",
    "\n",
    "        while index < length:\n",
    "            if diarization_timestamps[index + 1][2] == diarization_timestamps[index][2] and diarization_timestamps[index + 1][0] - threshold <= diarization_timestamps[index][0]:\n",
    "                diarization_timestamps[index][1] = diarization_timestamps[index + 1][1]\n",
    "                del diarization_timestamps[index + 1]\n",
    "                length -= 1\n",
    "            else:\n",
    "                index += 1\n",
    "    return diarization_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3150db8-ece0-495d-90a6-0239e869bff1",
   "metadata": {},
   "source": [
    "<br> Finally, we try to extand the timestamps to avoid word cutting. Imagine we have a segment [00:01:20 --> 00:01:25], followed by [00:01:27 --> 00:01:30].\n",
    "<br> Maybe diarizations is not working fine and there is some sound missing in the segments (means missing sound is between 00:01:25 and 00:01:27). Solution consists in fixing the end of the first segment and the start of the second one to 00:01:26, the middle of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74152746-07a6-46cf-8421-4f60ad0e5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extanding_timestamps(new_diarization_timestamps):\n",
    "    \"\"\"\n",
    "    Extanding timestamps between each diarization if possible, so we avoid word cutting\n",
    "    :param new_diarization_timestamps: list\n",
    "    :return: list with merged times\n",
    "    \"\"\"\n",
    "    for i in range(1, len(new_diarization_timestamps)):\n",
    "        if new_diarization_timestamps[i][0] - new_diarization_timestamps[i - 1][1] <= timedelta(milliseconds=3000) and new_diarization_timestamps[i][0] - new_diarization_timestamps[i - 1][1] >= timedelta(milliseconds=100):\n",
    "            middle = (new_diarization_timestamps[i][0] - new_diarization_timestamps[i - 1][1]) / 2\n",
    "            new_diarization_timestamps[i][0] -= middle\n",
    "            new_diarization_timestamps[i - 1][1] += middle\n",
    "\n",
    "    # Converting list so we have a milliseconds format\n",
    "    for elt in new_diarization_timestamps:\n",
    "        elt[0] = elt[0].total_seconds() * 1000\n",
    "        elt[1] = elt[1].total_seconds() * 1000\n",
    "\n",
    "    return new_diarization_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39eedc-b36f-476d-9b43-9698d151fff4",
   "metadata": {},
   "source": [
    "Let's apply these three functions to the result of our diarization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c353f6f5-0205-4136-bf89-875c668a6249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[210.0, 1590.0, 'Speaker1'], [1590.0, 25505.0, 'Speaker0'], [25505.0, 54840.0, 'Speaker1']]\n"
     ]
    }
   ],
   "source": [
    "if len(diarization) > 0:\n",
    "    diarization_timestamps = convert_dia_results_to_timedelta(diarization, start=0)\n",
    "    diarization_timestamps = merge_speaker_times(diarization_timestamps, max_space=45000)\n",
    "    diarization_timestamps = extanding_timestamps(diarization_timestamps)\n",
    "    \n",
    "print(diarization_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dac608-4c1f-4e2b-883c-d009f139141b",
   "metadata": {},
   "source": [
    "Now that we have merged the timestamps, we can transcribe our audio!\n",
    "<br>Here, we are going to create a timestamped transcript by converting the considered timestamps in displayed timedelta objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a134c4a-5a53-443b-b441-2bff6dd52b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 --> 0:00:01\n",
      "\n",
      "Speaker1: o admire over to you\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01 --> 0:00:25\n",
      "\n",
      "Speaker0: akes markus well i've prepared some handouts to show you how the figures are looking if we look at sails year to date so in order to meet budget this year we will in my opinion have to start some cost saving measures what do you mean by that redundancies that's one possibility\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:25 --> 0:00:54\n",
      "\n",
      "Speaker1: i can't agree with you there we need a strong work for us oka thanks myr the cos cutting is something that we need to think about but we also need to stop the downturn in sales and regain marketshire so can i bring you in here david any comments well it's hard to know what the problem is our products are competitive so they're not getting better offers from the competition i don't think so\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(diarization_timestamps[0], list):  # Only the case if only one \"list\" in the list (it makes a classic list) not a list of list\n",
    "    diarization_timestamps = [diarization_timestamps]\n",
    "    \n",
    "save_result_list = []\n",
    "for index, elt in enumerate(diarization_timestamps):\n",
    "    sub_start = elt[0]\n",
    "    sub_end = elt[1]\n",
    "    \n",
    "    temp_transcript = transcribe_audio_part(filename, stt_model, stt_tokenizer, conversation_audio, sub_start, sub_end,\n",
    "                                              index)\n",
    "    \n",
    "    temp_timestamps = str(timedelta(milliseconds=sub_start)).split(\".\")[0] + \" --> \" + \\\n",
    "                              str(timedelta(milliseconds=sub_end)).split(\".\")[0] + \"\\n\"\n",
    "    complete_temp_transcript = elt[2] + \": \" + temp_transcript\n",
    "    \n",
    "    print(temp_timestamps)\n",
    "    print(complete_temp_transcript+ \"\\n\\n\")\n",
    "    \n",
    "    # Add result of transcript to a list, so we can use it after\n",
    "    save_result_list.append([temp_timestamps, int(elt[2][-1]), elt[2], \" : \" + temp_transcript, int(sub_start / 1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2b1e1-ab21-4d2f-9e81-99a4ca695c5e",
   "metadata": {},
   "source": [
    "Result seems good, but if you listen to the audio, you will notice that it is not perfect. Indeed, this audio contains more than 2 speakers which means pyannote has confused interlocutors. \n",
    "<Br>However, it is important to remember that our audio is only one minute long, which is very hard to analyse because each speaker does not have so much time to speak. The error rate is therefore more important. We need more data so it will work better if the file is longer.\n",
    "    \n",
    "=> Peut-être plutôt faire cette partie sur un audio + long pour montrer que ça peut mieux fonctionner justement? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "162d90eb-0d9e-4abd-bbfa-5926a7055feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parfois il y a un warning huggingface/tokenizers: The current process just got forked, after parallelism has already been used.\n",
    "\n",
    "# peut être résolu par les 2 lignes suivantes\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = false / true ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f0e04-48c4-4fb0-9475-83da04f637cd",
   "metadata": {},
   "source": [
    "### B - Rename speakers\n",
    "Of course, it would be interesting to have the possibility to rename the detected speakers in the audio file. Indeed, having Speaker0, Speaker1, ... is fine but it could be so much better! Guess what? We are going to do this! \n",
    "<br>We are going to create a list where we add each speaker with his 'ID'. It is important to sort the list because we want the smallest ID at the beginning of our list, so we don't exchange names between speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c67a662-6ce6-472e-a724-265d49879438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'Speaker0'], [1, 'Speaker1']]\n"
     ]
    }
   ],
   "source": [
    "if number_of_speakers > 0:\n",
    "    # Saving the Speaker Name and its ID in a list, example : [1, 'Speaker1']\n",
    "    list_of_speakers = []\n",
    "    for elt in save_result_list:        \n",
    "        if [elt[1], elt[2]] not in list_of_speakers:\n",
    "            list_of_speakers.append([elt[1], elt[2]])\n",
    "\n",
    "    # Sorting (by ID)\n",
    "    list_of_speakers.sort()  # Sort id by ID so we do not have an unsorted case like [1, 'Speaker1'], [0, 'Speaker0']]\n",
    "    print(list_of_speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33098872-1e91-4fe5-b4f6-3ebc347b66ca",
   "metadata": {},
   "source": [
    "As you can see, we have the two detected speakers with their respective name and ID.\n",
    "Now, let's rename the names of our speakers as 'You' and 'Me' like we are having a conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "173630a5-2551-4fcc-9f24-649964ba3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the name_index value. It indicates the index of the name if the list_of_speakers list.\n",
    "name_index = 1\n",
    "\n",
    "# Change names\n",
    "list_of_speakers[0][name_index] = \"You\"\n",
    "list_of_speakers[1][name_index] = \"Me\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9200e4e-1f24-481a-8316-f38438485405",
   "metadata": {},
   "source": [
    "Everything is ready! We just have to apply the changes to our initial save_result list, and print the transcript again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07517b32-3f53-43ae-a945-995692b45b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 --> 0:00:01\n",
      "Me : o admire over to you\n",
      "\n",
      "0:00:01 --> 0:00:25\n",
      "You : akes markus well i've prepared some handouts to show you how the figures are looking if we look at sails year to date so in order to meet budget this year we will in my opinion have to start some cost saving measures what do you mean by that redundancies that's one possibility\n",
      "\n",
      "0:00:25 --> 0:00:54\n",
      "Me : i can't agree with you there we need a strong work for us oka thanks myr the cos cutting is something that we need to think about but we also need to stop the downturn in sales and regain marketshire so can i bring you in here david any comments well it's hard to know what the problem is our products are competitive so they're not getting better offers from the competition i don't think so\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for elt in save_result_list:\n",
    "    elt[2] = list_of_speakers[elt[1]][1]\n",
    "    print(elt[0], elt[2] + elt[3] + \"\\n\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdd3a5-0b72-4035-957d-68066a45f2bc",
   "metadata": {},
   "source": [
    "Congratulations, you now have the transcript of a conversation between your own interlocutors with their speaking timestamps! \n",
    "\n",
    "# Step 8 : Create a SRT transcript (subtitles for videos)\n",
    "\n",
    "We just saw that we can create a timestamped transcript instead of creating a simple transcript.txt. That could help us to create subtitles for videos for your viewers or even deaf persons! \n",
    "<br>Idea is very simple here, process is the same as before. We just have to shorten the timestamps, so the transcript is synchronized with the video. Indeed, a 30s timestamp will be accompanied by a lot of text as shown in the left figure below, which spoils the visual effect of the subtitle synchronization. We want to have 10s maximum timestamps, which means short texts which each will move to the next one, like the right figure below.\n",
    "\n",
    "![title](images/srt_figure.png)\n",
    "\n",
    "<br><br>To do that, we just have to modify the minimum and maximum length (minimum and max_space) values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67cff0e2-129c-4c9c-bb17-57e7302dec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny time intervals if we want a short text\n",
    "min_space = 1000  # 1 sec\n",
    "max_space = 8000  # 8 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4fbc0-b8bc-4745-9e6d-2ff3c455f7e5",
   "metadata": {},
   "source": [
    "Once this is done, we just have to set the srt_token to True, to adapt the code and create short intervals instead of the largest ones as possible. Then, you can either choose the silence detection or the diarization method. SRT transcription works with both, but it makes more sense if you opt for the silence detection, as it is faster to process and does not give you the information about who is speaking when, thing that we do not need because of the video.\n",
    "\n",
    "### Trim an audio\n",
    "Let's do this on obama's speech again! (but only on the first minute of speech so we don't get a massive output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28c1297f-69ab-4df7-a575-f5a470ca232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "\n",
    "# Trim from 0s to 58s\n",
    "start = 0\n",
    "end = 58\n",
    "\n",
    "obama_audio = AudioSegment.from_file(\"/workspace/Speech_to_Text/sounds/obama.mp3\")\n",
    "obama_audio = obama_audio[start*1000:end*1000]  # Works in milliseconds (*1000)\n",
    "\n",
    "srt_token = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac16ab7-c6bb-4fab-b976-22e2da4717e9",
   "metadata": {},
   "source": [
    "### Generate .srt transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ea62974-f29b-4ba6-9f7e-d15e4f998e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/workspace/.miniconda3/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0:00:00 --> 0:00:03\n",
      "por even as we celebrate to night \n",
      "\n",
      "1\n",
      "0:00:03 --> 0:00:07\n",
      "we know the challenges that to morrow will bring \n",
      "\n",
      "2\n",
      "0:00:07 --> 0:00:10\n",
      "are the greatest of our lifeton \n",
      "\n",
      "3\n",
      "0:00:10 --> 0:00:17\n",
      "i stand here knowing that my story is part of the larger american story that \n",
      "\n",
      "4\n",
      "0:00:17 --> 0:00:25\n",
      "i owe a debt to all of those who came before me and that in no other country on earth is my story even possible \n",
      "\n",
      "5\n",
      "0:00:25 --> 0:00:26\n",
      " \n",
      "\n",
      "6\n",
      "0:00:26 --> 0:00:34\n",
      "i am the son of a black man from canya and a white woman from kansas i am married to a \n",
      "\n",
      "7\n",
      "0:00:34 --> 0:00:42\n",
      "black american who carries within her the blood of slaves and slave owners i have brothers sisters nieces nephews uncles \n",
      "\n",
      "8\n",
      "0:00:42 --> 0:00:50\n",
      "and cousins of every race and every hue scattered across three countenents these people are part of me \n",
      "\n",
      "9\n",
      "0:00:50 --> 0:00:58\n",
      "and they are part of america this country that i love \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# detect silences and silences treatment\n",
    "srt_silence_list = detect_silences(obama_audio)\n",
    "srt_new_silence_list=get_middle_values(srt_silence_list)\n",
    "srt_new_silence_list2 = silences_distribution(srt_new_silence_list, min_space, max_space, start, end, srt_token)\n",
    "\n",
    "# transcribe audio and obtain subtitles\n",
    "srt_transcript = \"\"\n",
    "for i in range(0, len(srt_new_silence_list2) - 1):\n",
    "    sub_start = srt_new_silence_list2[i]\n",
    "    sub_end = srt_new_silence_list2[i + 1]\n",
    "\n",
    "    # Initial audio has been split with start & end values\n",
    "    # It begins to 0s, but the timestamps need to be adjust with +start*1000 values to adapt the gap\n",
    "\n",
    "    transcript = transcribe_audio_part(filename, stt_model, stt_tokenizer, obama_audio, sub_start, sub_end, i) + \" \"\n",
    "    srt_transcript += str(i) + \"\\n\" + str(timedelta(milliseconds=sub_start)).split(\".\")[0] + \" --> \" + str(timedelta(milliseconds=sub_end)).split(\".\")[0] + \"\\n\" + transcript + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7efdad-d257-449e-897c-a4fa2c19eedf",
   "metadata": {},
   "source": [
    "Are you ready? Let's take a look at our subtitles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc32c8bc-37c6-40ca-9b87-4e5aeb6f0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0:00:00 --> 0:00:03\n",
      "por even as we celebrate to night \n",
      "\n",
      "1\n",
      "0:00:03 --> 0:00:07\n",
      "we know the challenges that to morrow will bring \n",
      "\n",
      "2\n",
      "0:00:07 --> 0:00:10\n",
      "are the greatest of our lifeton \n",
      "\n",
      "3\n",
      "0:00:10 --> 0:00:17\n",
      "i stand here knowing that my story is part of the larger american story that \n",
      "\n",
      "4\n",
      "0:00:17 --> 0:00:25\n",
      "i owe a debt to all of those who came before me and that in no other country on earth is my story even possible \n",
      "\n",
      "5\n",
      "0:00:25 --> 0:00:26\n",
      " \n",
      "\n",
      "6\n",
      "0:00:26 --> 0:00:34\n",
      "i am the son of a black man from canya and a white woman from kansas i am married to a \n",
      "\n",
      "7\n",
      "0:00:34 --> 0:00:42\n",
      "black american who carries within her the blood of slaves and slave owners i have brothers sisters nieces nephews uncles \n",
      "\n",
      "8\n",
      "0:00:42 --> 0:00:50\n",
      "and cousins of every race and every hue scattered across three countenents these people are part of me \n",
      "\n",
      "9\n",
      "0:00:50 --> 0:00:58\n",
      "and they are part of america this country that i love \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(srt_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448d91b-9bfc-4b86-b8d5-61a86894360e",
   "metadata": {},
   "source": [
    "### Export timestamped transcript as .srt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed04bc-965b-4bfb-b1fa-0cb391763fc1",
   "metadata": {},
   "source": [
    "Now, you can export this timestamped transcript in a .srt file. Then, you will be able to import this file on a compatible media player software and follow your video with subtitles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af87c9a8-8e13-4f94-b702-0b7e75763542",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/Speech_to_Text/saved_transcripts/transcript.srt', 'w+') as f:\n",
    "    f.write(srt_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d81ec0-4004-4488-8650-86d40550d6f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Your transcript is now saved! \n",
    "\n",
    "## Conclusion\n",
    "<br>I hope you have enjoyed this tutorial!\n",
    "<br>You are now capable of creating a transcript with a speaker differentiation (diarization) and a timestamped transcript to generate video's subtitles!\n",
    "<br><br>You can check our **next tutorials** by clicking here to learn how to:\n",
    "\n",
    "<ul>\n",
    "    <li>Compare models and choose the best one</li>\n",
    "    <li>Build a complete Streamlit application to make your code interactive.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
